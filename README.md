# build-kg

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![CI](https://github.com/agtm1199/build-kg/actions/workflows/ci.yml/badge.svg)](https://github.com/agtm1199/build-kg/actions/workflows/ci.yml)

**One command. Any topic. Knowledge graph in your own PostgreSQL.**

build-kg turns any topic into a structured knowledge graph stored in Apache AGE (PostgreSQL). No vendor lock-in. No hosting fees. Just your own database.

```
/build-kg kubernetes networking

Claude autonomously:
  -> Generates an ontology (Component, Concept, Configuration)
  -> Researches authoritative sources (5-round discovery)
  -> Crawls official documentation
  -> Chunks documents by section boundaries
  -> Loads to PostgreSQL
  -> Parses with LLM into your graph
  -> Outputs: queryable knowledge graph in Apache AGE
```

## The Problem

Building knowledge graphs is hard. The #1 pain point? **Ontology design** — figuring out the right node types, relationships, and properties for your domain. Existing tools either lock you into their hosted platform or require you to define everything upfront.

**build-kg solves both.** Claude automatically generates the ontology for your topic, and the graph lives in your own PostgreSQL. No vendor, no fees.

## What You Get

### Any Topic

```
/build-kg React architecture patterns
/build-kg machine learning optimization algorithms
/build-kg GDPR compliance requirements
/build-kg kubernetes networking
```

### Automatic Ontology Generation

For **"kubernetes networking"**, Claude generates:
```
Component              "kube-proxy"
  |                    type: proxy, layer: L4
  |
  +-- USES ----------> Concept: "iptables"
  |                    category: packet filtering
  |
  +-- CONFIGURES ----> Configuration: "service.spec.type"
                       default_value: ClusterIP, scope: service
```

For **regulatory topics**, a specialized ontology is used:
```
Provision              "Sodium content must be declared per serving"
  |                    authority: CFIA, jurisdiction: CA
  |
  +-- Requirement      type: labelling, modality: must
  |     |
  |     +-- Constraint logic_type: threshold
  |                    target_signal: product.sodium
  |                    operator: <=, threshold: 140, unit: mg
```

Every constraint is machine-testable: thresholds with operators and units, regex patterns, enumerations, boolean flags.

## How It's Different

| | build-kg | LangChain/LlamaIndex | Neo4j Aura |
|---|---------|---------------------|-----------|
| **Graph hosting** | Your own PostgreSQL | Their infra | Neo4j cloud ($$$) |
| **Vendor lock-in** | None — it's just AGE | Their SDK | Their platform |
| **Ontology** | Auto-generated by Claude | You define it | You define it |
| **Interface** | `/build-kg` in Claude Code | Python code | Dashboard/API |
| **Cost** | LLM API calls only (~$0.03/100 fragments) | LLM + hosting | LLM + hosting + platform |

## Features

- **Full automation** — one Claude Code skill command runs the entire pipeline end-to-end
- **Automatic ontology generation** — Claude designs the graph structure for your topic
- **Any topic** — regulatory, technical, scientific, or anything else
- **Async web crawler** with configurable depth, delay, and page limits
- **Smart document chunking** that respects document structure (headings, sections)
- **LLM-powered parsing** that extracts entities and relationships into graph nodes
- **Batch API support** for 50% cheaper processing of large datasets
- **Apache AGE graph database** — query with Cypher, backed by PostgreSQL
- **Multi-domain support** via YAML domain profiles with pre-built ontologies
- **Custom profiles** — create your own domain profile with a custom ontology
- **Self-hosted** — no vendor dependencies, no hosting fees

## Quickstart

### Prerequisites

- [Claude Code](https://docs.anthropic.com/en/docs/claude-code)
- **Docker** (for PostgreSQL + Apache AGE)
- **Anthropic API key** (default) or OpenAI API key (for text parsing)

### Install

```bash
git clone https://github.com/agtm1199/build-kg.git
cd build-kg
make setup
```

```bash
# Configure
cp .env.example .env
# Edit .env and set ANTHROPIC_API_KEY=sk-ant-...

# Verify everything works
make verify
```

## Usage

In Claude Code, the `/build-kg` skill automates the entire pipeline:

```
/build-kg kubernetes networking
/build-kg full regulatory compliance landscape of Singapore for F&B
/build-kg machine learning optimization algorithms
/build-kg GDPR compliance requirements for SaaS companies
```

This runs all phases autonomously — generating the ontology, researching sources, crawling, chunking, loading, parsing, and reporting.

### With CLI Tools

Run each step manually for full control:

```bash
# 1. Crawl a website
build-kg-crawl --url "https://example.com/docs" --depth 2 --pages 50 --output ./output/

# 2. Chunk the crawled content
build-kg-chunk ./output/ ./chunks/ --strategy by_title --max-chars 1000

# 3. Load chunks to database
build-kg-load ./chunks/ --manifest ./crawl_manifest.json

# 4. Parse fragments into the knowledge graph
AGE_GRAPH_NAME=kg_k8s_net build-kg-parse --ontology ./ontology.yaml

# 5. Or use the batch API (50% cheaper for large runs)
build-kg-parse-batch prepare --ontology ./ontology.yaml
build-kg-parse-batch submit batch_data/batch_requests.jsonl
build-kg-parse-batch status <batch_id> --watch
build-kg-parse-batch process <batch_id> --ontology ./ontology.yaml
```

## Pipeline

```
Phase 0       Phase 0.5      Phase 1        Phase 2        Phase 3        Phase 4        Phase 5
INIT          ONTOLOGY       DISCOVER       CRAWL          CHUNK          LOAD           PARSE
--------      --------       --------       --------       --------       --------       --------
Set graph     Auto-gen       WebSearch      crawl.py       chunk.py       load.py        parse.py
name, dirs    ontology       WebFetch       (Crawl4AI)     (Unstructured) (PostgreSQL)   (Claude Haiku 3.5)
              or load
              from profile
```

## Domain Profiles

build-kg supports specialized domains through YAML profiles. Each profile configures the ontology, LLM prompt, ID extraction patterns, and source discovery templates.

### Built-in Profiles

| Profile | Domain | Description |
|---------|--------|-------------|
| `default` | Generic | Any topic — ontology auto-generated by Claude |
| `food-safety` | Food & Beverage | CFIA, FDA, SFA food safety and labeling |
| `financial-aml` | Financial AML/KYC | Anti-money laundering, customer due diligence |
| `data-privacy` | Data Protection | GDPR, CCPA, privacy regulations |

### Selecting a Domain

```bash
# Via CLI flag
build-kg-parse --domain financial-aml

# Via environment variable
DOMAIN=financial-aml build-kg-parse --jurisdiction US

# Custom profile file
build-kg-parse --domain /path/to/my-custom-domain.yaml
```

### Custom Profiles

Create a YAML file following the profile schema. Profiles can include a pre-built ontology or leave it for auto-generation:

```bash
build-kg-parse --domain /path/to/my-custom-domain.yaml
```

See `src/build_kg/domains/food-safety.yaml` for a complete example with ontology, or the [Contributing guide](CONTRIBUTING.md#adding-a-new-domain-profile) for instructions.

```bash
# List all available profiles
build-kg-domains
```

## CLI Reference

| Command | Description | Docs |
|---------|-------------|------|
| `build-kg-crawl` | Crawl websites to markdown/HTML/JSON | [reference-crawl.md](docs/reference-crawl.md) |
| `build-kg-chunk` | Chunk documents into JSON fragments | [reference-chunk.md](docs/reference-chunk.md) |
| `build-kg-load` | Load chunks into PostgreSQL with manifest | [reference-load.md](docs/reference-load.md) |
| `build-kg-parse` | Parse fragments with Claude Haiku 3.5 (sync) | [reference-parse.md](docs/reference-parse.md) |
| `build-kg-parse-batch` | Parse fragments with Batch API | [reference-parse.md](docs/reference-parse.md) |
| `build-kg-setup` | Create AGE extension and graph | -- |
| `build-kg-verify` | Verify database, AGE, and API setup | -- |
| `build-kg-domains` | List available domain profiles | -- |

## Configuration

All configuration is via `.env` file or environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `DB_HOST` | `localhost` | PostgreSQL host |
| `DB_PORT` | `5432` | PostgreSQL port |
| `DB_NAME` | `buildkg` | Database name |
| `DB_USER` | `buildkg` | Database user |
| `DB_PASSWORD` | -- | Database password (**required**) |
| `LLM_PROVIDER` | `anthropic` | LLM provider (`anthropic` or `openai`) |
| `ANTHROPIC_API_KEY` | -- | Anthropic API key (**required** if provider is `anthropic`) |
| `ANTHROPIC_MODEL` | `claude-haiku-4-5-20251001` | Anthropic model for parsing |
| `OPENAI_API_KEY` | -- | OpenAI API key (**required** if provider is `openai`) |
| `OPENAI_MODEL` | `gpt-4o-mini` | OpenAI model for parsing |
| `AGE_GRAPH_NAME` | `reg_ca` | Apache AGE graph name |
| `BATCH_SIZE` | `10` | Fragments per batch |
| `MAX_WORKERS` | `3` | Concurrent workers |
| `DOMAIN` | `food-safety` | Domain profile name or path |
| `RATE_LIMIT_DELAY` | `1.0` | Seconds between API calls |

## Cost

The only cost is LLM API calls during parsing. Everything else runs locally.

| Fragments | Sync Parser | Batch Parser (50% off) |
|-----------|------------|----------------------|
| 100 | ~$0.03 | ~$0.015 |
| 1,000 | ~$0.30 | ~$0.15 |
| 5,000 | ~$1.50 | ~$0.75 |

Crawling, chunking, loading, and graph queries are free.

## Examples

See [`examples/manifests/`](examples/manifests/) for sample crawl manifests:

- **[singapore-fb.json](examples/manifests/singapore-fb.json)** -- Singapore F&B regulations (12 sources across SFA, Parliament, HPB, MUIS)
- **[canada-food.json](examples/manifests/canada-food.json)** -- Canadian food regulations (3 core sources)

## Documentation

| Doc | Description |
|-----|-------------|
| [HTML Usage Guide](docs/guide.html) | Comprehensive single-page guide with all sections |
| [Architecture](docs/architecture.md) | Pipeline design, ontology system, database schema, batch vs sync tradeoffs |
| [Quickstart Tutorial](docs/tutorial-quickstart.md) | Zero to working graph in 5 minutes |
| [Singapore Tutorial](docs/tutorial-singapore.md) | Full worked example with real regulatory data (12 sources, 2,400 provisions) |
| [Manifest Format](docs/manifest-format.md) | JSON schema for crawl manifests (regulatory and generic) |
| [Troubleshooting](docs/troubleshooting.md) | Common errors and fixes |

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for setup instructions and guidelines.

Some ways to contribute:
- **Add a domain profile** for your area (pharma, environmental, telecom, etc.)
- **Add ID extraction patterns** for your jurisdiction's regulatory ID format
- **Improve the pipeline** with better chunking strategies, parsing prompts, or graph enrichment
- **Report bugs** and suggest features via [GitHub Issues](https://github.com/agtm1199/build-kg/issues)

## License

Apache 2.0 -- see [LICENSE](LICENSE) for details.
