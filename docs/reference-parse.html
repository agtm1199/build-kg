<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>CLI Reference: Parsers - build-kg</title>
<style>
:root { --bg: #0d1117; --surface: #161b22; --border: #30363d; --text: #c9d1d9; --text-muted: #8b949e; --heading: #f0f6fc; --link: #58a6ff; --accent: #1f6feb; --green: #3fb950; --orange: #d29922; --red: #f85149; --purple: #bc8cff; --code-bg: #0d1117; --inline-code-bg: #1c2128; }
* { margin: 0; padding: 0; box-sizing: border-box; }
body { font-family: 'JetBrains Mono', 'Fira Code', 'SF Mono', 'Cascadia Code', 'Consolas', 'Liberation Mono', monospace; background: var(--bg); color: var(--text); line-height: 1.6; font-size: 16px; }
.container { max-width: 960px; margin: 0 auto; padding: 2rem 1.5rem; }
.hero { text-align: center; padding: 3rem 0; border-bottom: 1px solid var(--border); margin-bottom: 2rem; }
.hero h1 { font-size: 2.5rem; color: var(--heading); margin-bottom: 0.5rem; letter-spacing: -0.02em; }
.hero .tagline { font-size: 1.2rem; color: var(--text-muted); margin-bottom: 1.5rem; }
.hero .badges { display: flex; gap: 0.5rem; justify-content: center; flex-wrap: wrap; }
.badge { display: inline-block; padding: 0.25rem 0.75rem; border-radius: 2rem; font-size: 0.8rem; font-weight: 600; border: 1px solid var(--border); color: var(--text-muted); }
.badge.green { border-color: var(--green); color: var(--green); }
.badge.blue { border-color: var(--link); color: var(--link); }
.badge.purple { border-color: var(--purple); color: var(--purple); }
section { margin-bottom: 3rem; padding-bottom: 2rem; border-bottom: 1px solid var(--border); }
section:last-child { border-bottom: none; }
h2 { font-size: 1.8rem; color: var(--heading); margin-bottom: 1rem; padding-top: 1rem; }
h3 { font-size: 1.3rem; color: var(--heading); margin: 1.5rem 0 0.75rem; }
h4 { font-size: 1.05rem; color: var(--heading); margin: 1.25rem 0 0.5rem; }
p { margin-bottom: 1rem; } ul, ol { margin-bottom: 1rem; padding-left: 1.5rem; } li { margin-bottom: 0.25rem; }
a { color: var(--link); text-decoration: none; } a:hover { text-decoration: underline; }
pre { background: var(--code-bg); border: 1px solid var(--border); border-radius: 6px; padding: 1rem 1.25rem; overflow-x: auto; margin: 1rem 0; font-size: 0.875rem; line-height: 1.5; }
code { font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace; font-size: 0.875em; }
:not(pre) > code { background: var(--inline-code-bg); padding: 0.15em 0.4em; border-radius: 3px; font-size: 0.85em; }
table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
th, td { text-align: left; padding: 0.6rem 0.8rem; border: 1px solid var(--border); }
th { background: var(--surface); color: var(--heading); font-weight: 600; }
tr:nth-child(even) { background: rgba(22, 27, 34, 0.5); }
footer { text-align: center; padding: 2rem 0; color: var(--text-muted); font-size: 0.85rem; }
@media (max-width: 700px) { .hero h1 { font-size: 2rem; } }
</style>
</head>
<body>

<nav style="position:sticky;top:0;z-index:100;background:var(--surface);border-bottom:1px solid var(--border);padding:0.75rem 1.5rem;display:flex;align-items:center;gap:1.5rem;font-size:0.9rem;backdrop-filter:blur(8px);">
  <a href="index.html" style="color:var(--heading);font-weight:700;text-decoration:none;">build-<span style="color:var(--link)">kg</span></a>
  <a href="documentation.html">Docs</a>
  <a href="guide.html">Guide</a>
  <a href="reference-parse.html" style="color:var(--heading);">Parsers</a>
  <a href="https://github.com/agtm1199/build-kg" style="margin-left:auto;">GitHub</a>
</nav>

<div class="container">

  <div class="hero">
    <h1>build-kg</h1>
    <p class="tagline">CLI Reference: Parsers</p>
    <div class="badges">
      <span class="badge green">GPT-4o-mini</span>
      <span class="badge blue">Sync &amp; Batch</span>
      <span class="badge purple">Ontology-Driven</span>
    </div>
  </div>

  <p>build-kg provides two parsers for converting text fragments into Apache AGE graph nodes. Both support two modes: <strong>ontology-driven</strong> (generic, any topic) and <strong>regulatory</strong> (legacy, Provision/Requirement/Constraint). Both produce the same quality of graph output but differ in cost, latency, and workflow.</p>

  <hr>

  <!-- ============================================================ -->
  <!-- Sync Parser -->
  <!-- ============================================================ -->
  <section>
    <h2>Sync Parser: build-kg-parse</h2>

    <p>Real-time parsing using the OpenAI Chat Completions API. Each fragment is sent to GPT-4o-mini individually, and the result is loaded into the graph immediately.</p>

    <h3>Usage</h3>

<pre><code>build-kg-parse [OPTIONS]</code></pre>

    <p>Or via Python module:</p>

<pre><code>python -m build_kg.parse [OPTIONS]</code></pre>

    <h3>Options</h3>

    <table>
      <thead>
        <tr><th>Flag</th><th>Default</th><th>Description</th></tr>
      </thead>
      <tbody>
        <tr><td><code>--limit N</code></td><td>all</td><td>Maximum number of fragments to process</td></tr>
        <tr><td><code>--offset N</code></td><td><code>0</code></td><td>Skip the first N fragments</td></tr>
        <tr><td><code>--jurisdiction CODE</code></td><td>all</td><td>Filter by jurisdiction code (e.g., <code>SG</code>, <code>CA</code>, <code>US</code>)</td></tr>
        <tr><td><code>--test</code></td><td>off</td><td>Test mode: process only 5 fragments</td></tr>
        <tr><td><code>--domain NAME</code></td><td><code>food-safety</code></td><td>Domain profile name or path to custom YAML profile</td></tr>
        <tr><td><code>--ontology PATH</code></td><td>none</td><td>Path to an ontology YAML file (enables ontology-driven mode)</td></tr>
      </tbody>
    </table>

    <h3>Environment Variables</h3>

    <p>The parser reads these from <code>.env</code> or the environment:</p>

    <table>
      <thead>
        <tr><th>Variable</th><th>Default</th><th>Description</th></tr>
      </thead>
      <tbody>
        <tr><td><code>AGE_GRAPH_NAME</code></td><td><code>reg_ca</code></td><td>Apache AGE graph to load results into</td></tr>
        <tr><td><code>OPENAI_API_KEY</code></td><td><strong>(required)</strong></td><td>OpenAI API key</td></tr>
        <tr><td><code>OPENAI_MODEL</code></td><td><code>gpt-4o-mini</code></td><td>Model to use for parsing</td></tr>
        <tr><td><code>BATCH_SIZE</code></td><td><code>10</code></td><td>Number of fragments per internal batch (for progress reporting)</td></tr>
        <tr><td><code>MAX_WORKERS</code></td><td><code>3</code></td><td>Concurrent worker count (currently sequential)</td></tr>
        <tr><td><code>RATE_LIMIT_DELAY</code></td><td><code>1.0</code></td><td>Seconds to wait between API calls</td></tr>
        <tr><td><code>DOMAIN</code></td><td><code>food-safety</code></td><td>Domain profile name or path (overridden by <code>--domain</code> flag)</td></tr>
      </tbody>
    </table>

    <h3>Parsing Modes</h3>

    <h4>Ontology-Driven Mode</h4>

    <p>When <code>--ontology</code> is provided or the active domain profile has an ontology with <code>json_schema</code>, the parser uses the ontology to:</p>

    <ol>
      <li>Generate prompts that describe node types, edge types, and expected properties</li>
      <li>Parse the LLM&rsquo;s JSON output into entities and relationships</li>
      <li>Create graph vertices and edges dynamically based on the ontology labels</li>
    </ol>

    <p>The LLM is expected to return JSON in this format:</p>

<pre><code>{
  &quot;entities&quot;: [
    {&quot;_label&quot;: &quot;Component&quot;, &quot;name&quot;: &quot;kube-proxy&quot;, &quot;type&quot;: &quot;proxy&quot;},
    {&quot;_label&quot;: &quot;Concept&quot;, &quot;name&quot;: &quot;iptables&quot;, &quot;category&quot;: &quot;packet filtering&quot;}
  ],
  &quot;relationships&quot;: [
    {&quot;_label&quot;: &quot;USES&quot;, &quot;_from_index&quot;: 0, &quot;_to_index&quot;: 1}
  ]
}</code></pre>

    <h4>Regulatory Mode (Legacy)</h4>

    <p>When no ontology is provided and the domain profile doesn&rsquo;t have one, the parser uses the hardcoded Provision/Requirement/Constraint structure. This is the original regulatory parsing mode.</p>

    <h3>Examples</h3>

    <h4>Test with 5 fragments (regulatory)</h4>

<pre><code>build-kg-parse --test</code></pre>

    <h4>Parse with a custom ontology (generic topic)</h4>

<pre><code>AGE_GRAPH_NAME=kg_k8s_net build-kg-parse --ontology ./ontology.yaml --limit 100</code></pre>

    <h4>Parse 100 fragments for Singapore (regulatory)</h4>

<pre><code>AGE_GRAPH_NAME=reg_sg_fb build-kg-parse --domain food-safety --limit 100 --jurisdiction SG</code></pre>

    <h4>Parse with a different domain profile</h4>

<pre><code>build-kg-parse --domain financial-aml --jurisdiction US</code></pre>

    <h4>Parse all fragments, skip first 500</h4>

<pre><code>build-kg-parse --offset 500</code></pre>

    <h3>When to Use</h3>

    <ul>
      <li><strong>&lt;500 fragments</strong>: The sync parser is convenient for small to medium runs.</li>
      <li><strong>Debugging</strong>: Immediate feedback makes it easy to inspect results and iterate.</li>
      <li><strong>Interactive development</strong>: When you want to see results as they are produced.</li>
    </ul>

    <h3>ID Extraction (Regulatory Mode Only)</h3>

    <p>The sync parser includes a two-stage ID extraction pipeline for regulatory domains:</p>

    <ol>
      <li><strong>Regex extraction</strong> (free, instant): Tries authority-specific patterns (CFIA B.01.008, US CFR 21 CFR 101.61, Section/Chapter/Article references) against the fragment text and canonical_locator metadata.</li>
      <li><strong>LLM extraction</strong> (paid, via the parsing prompt): The GPT-4o-mini prompt asks for the provision ID as part of the structured output.</li>
    </ol>

    <p>The parser chooses the best ID source:</p>

    <ul>
      <li>Regex with confidence &gt;= 0.70: Use regex result</li>
      <li>Regex failed but LLM found an ID: Use LLM result</li>
      <li>Regex with any confidence and LLM failed: Use regex result</li>
      <li>Both failed: Set provision_id to <code>UNKNOWN</code></li>
    </ul>

    <p>In ontology-driven mode, ID extraction is skipped and auto-generated IDs are used.</p>
  </section>

  <hr>

  <!-- ============================================================ -->
  <!-- Batch Parser -->
  <!-- ============================================================ -->
  <section>
    <h2>Batch Parser: build-kg-parse-batch</h2>

    <p>Uses the OpenAI Batch API for 50% cheaper processing. Designed for large-scale runs (&gt;=500 fragments). The workflow has four subcommands that are run sequentially.</p>

    <h3>Usage</h3>

<pre><code>build-kg-parse-batch &lt;command&gt; [OPTIONS]</code></pre>

    <p>Or via Python module:</p>

<pre><code>python -m build_kg.parse_batch &lt;command&gt; [OPTIONS]</code></pre>

    <h3>Subcommands</h3>

    <h4><code>prepare</code> &mdash; Create a JSONL Batch File</h4>

    <p>Fetches fragments from the database and writes them as OpenAI Batch API requests in JSONL format.</p>

<pre><code>build-kg-parse-batch prepare [OPTIONS]</code></pre>

    <table>
      <thead>
        <tr><th>Flag</th><th>Default</th><th>Description</th></tr>
      </thead>
      <tbody>
        <tr><td><code>--limit N</code></td><td>all</td><td>Maximum number of fragments to include</td></tr>
        <tr><td><code>--offset N</code></td><td><code>0</code></td><td>Skip the first N fragments</td></tr>
        <tr><td><code>--jurisdiction CODE</code></td><td>all</td><td>Filter by jurisdiction code</td></tr>
        <tr><td><code>--output FILENAME</code></td><td><code>batch_requests.jsonl</code></td><td>Output JSONL filename (saved in <code>./batch_data/</code>)</td></tr>
        <tr><td><code>--domain NAME</code></td><td><code>food-safety</code></td><td>Domain profile name or path</td></tr>
        <tr><td><code>--ontology PATH</code></td><td>none</td><td>Path to ontology YAML file</td></tr>
      </tbody>
    </table>

    <p>Example:</p>

<pre><code>build-kg-parse-batch prepare --jurisdiction SG --output sg_batch.jsonl</code></pre>

    <p>Example (generic topic with ontology):</p>

<pre><code>build-kg-parse-batch prepare --ontology ./ontology.yaml --output k8s_batch.jsonl</code></pre>

    <p>Output files:</p>

    <ul>
      <li><code>batch_data/sg_batch.jsonl</code> &mdash; The JSONL file to submit to OpenAI</li>
      <li><code>batch_data/sg_batch.jsonl.metadata.json</code> &mdash; Fragment metadata for processing results later</li>
    </ul>

    <h4><code>submit</code> &mdash; Submit Batch to OpenAI</h4>

    <p>Uploads the JSONL file and creates a batch job.</p>

<pre><code>build-kg-parse-batch submit &lt;batch_file&gt;</code></pre>

    <table>
      <thead>
        <tr><th>Argument</th><th>Description</th></tr>
      </thead>
      <tbody>
        <tr><td><code>batch_file</code></td><td>Path to the JSONL file created by <code>prepare</code></td></tr>
      </tbody>
    </table>

    <p>Example:</p>

<pre><code>build-kg-parse-batch submit batch_data/sg_batch.jsonl</code></pre>

    <p>Output:</p>

    <ul>
      <li>Prints the batch ID (e.g., <code>batch_abc123def456</code>)</li>
      <li>Saves batch info to <code>batch_data/batch_&lt;batch_id&gt;.info.json</code></li>
    </ul>

    <h4><code>status</code> &mdash; Check Batch Status</h4>

    <p>Checks the current status of a submitted batch.</p>

<pre><code>build-kg-parse-batch status &lt;batch_id&gt; [OPTIONS]</code></pre>

    <table>
      <thead>
        <tr><th>Argument/Flag</th><th>Description</th></tr>
      </thead>
      <tbody>
        <tr><td><code>batch_id</code></td><td>The OpenAI batch ID from the <code>submit</code> step</td></tr>
        <tr><td><code>--watch</code></td><td>Poll every 60 seconds until the batch completes</td></tr>
      </tbody>
    </table>

    <p>Example (one-shot):</p>

<pre><code>build-kg-parse-batch status batch_abc123def456</code></pre>

    <p>Example (watch mode):</p>

<pre><code>build-kg-parse-batch status batch_abc123def456 --watch</code></pre>

    <p>Possible statuses:</p>

    <ul>
      <li><code>validating</code> &mdash; OpenAI is validating the input file</li>
      <li><code>in_progress</code> &mdash; Requests are being processed</li>
      <li><code>completed</code> &mdash; All requests finished</li>
      <li><code>failed</code> &mdash; The batch failed (check errors)</li>
      <li><code>expired</code> &mdash; The batch expired (24-hour window exceeded)</li>
      <li><code>cancelled</code> &mdash; The batch was cancelled</li>
    </ul>

    <h4><code>process</code> &mdash; Download Results and Load to Graph</h4>

    <p>Downloads the completed batch results and loads each parsed entity into the Apache AGE graph.</p>

<pre><code>build-kg-parse-batch process &lt;batch_id&gt; [OPTIONS]</code></pre>

    <table>
      <thead>
        <tr><th>Argument/Flag</th><th>Description</th></tr>
      </thead>
      <tbody>
        <tr><td><code>batch_id</code></td><td>The OpenAI batch ID from the <code>submit</code> step</td></tr>
        <tr><td><code>--ontology PATH</code></td><td>Path to ontology YAML file (must match the one used in <code>prepare</code>)</td></tr>
      </tbody>
    </table>

    <p>Example (regulatory):</p>

<pre><code>AGE_GRAPH_NAME=reg_sg_fb build-kg-parse-batch process batch_abc123def456</code></pre>

    <p>Example (generic with ontology):</p>

<pre><code>AGE_GRAPH_NAME=kg_k8s_net build-kg-parse-batch process batch_abc123def456 --ontology ./ontology.yaml</code></pre>

    <p>Output files:</p>

    <ul>
      <li><code>batch_data/batch_&lt;batch_id&gt;_results.jsonl</code> &mdash; Downloaded results</li>
      <li><code>batch_data/batch_&lt;batch_id&gt;_errors.jsonl</code> &mdash; Downloaded errors (if any)</li>
    </ul>

    <h3>Full Batch Workflow</h3>

    <h4>Regulatory topic</h4>

<pre><code># Step 1: Prepare
build-kg-parse-batch prepare --jurisdiction SG

# Step 2: Submit
build-kg-parse-batch submit batch_data/batch_requests.jsonl

# Step 3: Wait and monitor
build-kg-parse-batch status batch_abc123def456 --watch

# Step 4: Process results into graph
AGE_GRAPH_NAME=reg_sg_fb build-kg-parse-batch process batch_abc123def456</code></pre>

    <h4>Generic topic</h4>

<pre><code># Step 1: Prepare with ontology
build-kg-parse-batch prepare --ontology ./ontology.yaml

# Step 2: Submit
build-kg-parse-batch submit batch_data/batch_requests.jsonl

# Step 3: Wait and monitor
build-kg-parse-batch status batch_abc123def456 --watch

# Step 4: Process results with same ontology
AGE_GRAPH_NAME=kg_k8s_net build-kg-parse-batch process batch_abc123def456 --ontology ./ontology.yaml</code></pre>

    <h3>When to Use</h3>

    <ul>
      <li><strong>&gt;=500 fragments</strong>: The 50% cost savings become significant.</li>
      <li><strong>Overnight runs</strong>: Submit before end of day, process results the next morning.</li>
      <li><strong>Budget-sensitive projects</strong>: When minimizing API costs matters.</li>
    </ul>

    <h3>Batch Data Directory</h3>

    <p>All batch files are stored in <code>./batch_data/</code> relative to the current working directory:</p>

<pre><code>batch_data/
  batch_requests.jsonl                    # Prepared requests
  batch_requests.jsonl.metadata.json      # Fragment metadata
  batch_batch_abc123def456.info.json      # Submission info
  batch_batch_abc123def456_results.jsonl  # Downloaded results
  batch_batch_abc123def456_errors.jsonl   # Downloaded errors</code></pre>
  </section>

  <hr>

  <!-- ============================================================ -->
  <!-- Cost Comparison -->
  <!-- ============================================================ -->
  <section>
    <h2>Cost Comparison</h2>

    <table>
      <thead>
        <tr><th>Parser</th><th>Pricing Model</th><th>Cost per 1,000 Fragments</th><th>Best For</th></tr>
      </thead>
      <tbody>
        <tr><td><code>build-kg-parse</code> (sync)</td><td>Standard OpenAI API</td><td>~$0.30</td><td>Small runs (&lt;500), debugging, interactive use</td></tr>
        <tr><td><code>build-kg-parse-batch</code> (batch)</td><td>OpenAI Batch API (50% discount)</td><td>~$0.15</td><td>Large runs (&gt;=500), overnight processing</td></tr>
      </tbody>
    </table>

    <p>Cost estimates assume GPT-4o-mini with an average of ~500 input tokens and ~300 output tokens per fragment.</p>

    <h3>Cost Examples</h3>

    <table>
      <thead>
        <tr><th>Fragments</th><th>Sync Cost</th><th>Batch Cost</th><th>Savings</th></tr>
      </thead>
      <tbody>
        <tr><td>100</td><td>~$0.03</td><td>~$0.015</td><td>$0.015</td></tr>
        <tr><td>1,000</td><td>~$0.30</td><td>~$0.15</td><td>$0.15</td></tr>
        <tr><td>5,000</td><td>~$1.50</td><td>~$0.75</td><td>$0.75</td></tr>
        <tr><td>10,000</td><td>~$3.00</td><td>~$1.50</td><td>$1.50</td></tr>
      </tbody>
    </table>
  </section>

  <hr>

  <!-- ============================================================ -->
  <!-- Graph Output -->
  <!-- ============================================================ -->
  <section>
    <h2>Graph Output</h2>

    <p>Both parsers support two output modes depending on whether an ontology is provided.</p>

    <h3>Ontology-Driven Mode (Generic)</h3>

    <ul>
      <li><strong>Nodes Created</strong>: Dynamic, based on ontology definition. Each entity type (e.g., Component, Concept, Configuration) becomes a vertex label.</li>
      <li><strong>Edges Created</strong>: Dynamic, based on ontology definition (e.g., USES, CONFIGURES, DEPENDS_ON).</li>
      <li><strong>Properties</strong>: Dynamic, from the LLM output matching the ontology&rsquo;s property definitions.</li>
    </ul>

    <h3>Regulatory Mode (Legacy)</h3>

    <ul>
      <li><strong>Provision</strong>: One per fragment that has extractable requirements</li>
      <li><strong>Requirement</strong>: One per regulatory obligation found in the fragment</li>
      <li><strong>Constraint</strong>: One per testable condition on a requirement</li>
      <li><strong>DERIVED_FROM</strong>: Links a Requirement to its parent Provision</li>
      <li><strong>HAS_CONSTRAINT</strong>: Links a Requirement to its Constraint(s)</li>
    </ul>

    <h3>Skipped Fragments</h3>

    <p>Fragments are skipped (not loaded to graph) when:</p>

    <ul>
      <li>The LLM returns empty entities/requirements (the text contains no extractable knowledge)</li>
      <li>The LLM response is not valid JSON</li>
      <li>A graph insertion error occurs (the fragment is counted as failed)</li>
    </ul>
  </section>

  <hr>

  <!-- ============================================================ -->
  <!-- Troubleshooting -->
  <!-- ============================================================ -->
  <section>
    <h2>Troubleshooting</h2>

    <table>
      <thead>
        <tr><th>Problem</th><th>Solution</th></tr>
      </thead>
      <tbody>
        <tr><td>&quot;OpenAI API error 401&quot;</td><td>Check <code>OPENAI_API_KEY</code> in <code>.env</code></td></tr>
        <tr><td>&quot;No fragments found&quot;</td><td>Verify that <code>build-kg-load</code> ran successfully; check <code>source_fragment</code> table</td></tr>
        <tr><td>&quot;Graph loading failed&quot;</td><td>Check that the graph exists: <code>build-kg-setup</code></td></tr>
        <tr><td>Batch still in &quot;validating&quot; status</td><td>Wait &mdash; OpenAI batches take 1&ndash;24 hours</td></tr>
        <tr><td>Batch status &quot;failed&quot;</td><td>Check the error output; common cause is malformed JSONL</td></tr>
        <tr><td>Batch status &quot;expired&quot;</td><td>The 24-hour window elapsed; resubmit the batch</td></tr>
        <tr><td>&quot;Metadata file not found&quot; during process</td><td>Ensure you run <code>process</code> from the same directory where you ran <code>prepare</code></td></tr>
        <tr><td>High number of UNKNOWN provision IDs</td><td>Expected for jurisdictions without structured IDs; consider post-processing</td></tr>
        <tr><td>&quot;Ontology file not found&quot;</td><td>Check that the <code>--ontology</code> path is correct and the file exists</td></tr>
      </tbody>
    </table>
  </section>

  <footer>
    <p>build-kg &mdash; Apache 2.0 License &mdash; <a href="https://github.com/agtm1199/build-kg">GitHub</a></p>
  </footer>

</div>
</body>
</html>
