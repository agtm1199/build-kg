<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Architecture - build-kg</title>
<style>
  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --border: #30363d;
    --text: #c9d1d9;
    --text-muted: #8b949e;
    --heading: #f0f6fc;
    --link: #58a6ff;
    --accent: #1f6feb;
    --green: #3fb950;
    --orange: #d29922;
    --red: #f85149;
    --purple: #bc8cff;
    --code-bg: #0d1117;
    --inline-code-bg: #1c2128;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: 'JetBrains Mono', 'Fira Code', 'SF Mono', 'Cascadia Code', 'Consolas', 'Liberation Mono', monospace;
    background: var(--bg); color: var(--text); line-height: 1.6; font-size: 16px;
  }
  .container { max-width: 960px; margin: 0 auto; padding: 2rem 1.5rem; }
  .hero { text-align: center; padding: 3rem 0; border-bottom: 1px solid var(--border); margin-bottom: 2rem; }
  .hero h1 { font-size: 2.5rem; color: var(--heading); margin-bottom: 0.5rem; letter-spacing: -0.02em; }
  .hero .tagline { font-size: 1.2rem; color: var(--text-muted); margin-bottom: 1.5rem; }
  .hero .badges { display: flex; gap: 0.5rem; justify-content: center; flex-wrap: wrap; }
  .badge { display: inline-block; padding: 0.25rem 0.75rem; border-radius: 2rem; font-size: 0.8rem; font-weight: 600; border: 1px solid var(--border); color: var(--text-muted); }
  .badge.green { border-color: var(--green); color: var(--green); }
  .badge.blue { border-color: var(--link); color: var(--link); }
  .badge.purple { border-color: var(--purple); color: var(--purple); }
  .toc { background: var(--surface); border: 1px solid var(--border); border-radius: 6px; padding: 1.5rem 2rem; margin-bottom: 3rem; }
  .toc h2 { font-size: 1.1rem; color: var(--heading); margin-bottom: 1rem; text-transform: uppercase; letter-spacing: 0.05em; }
  .toc ol { list-style: none; counter-reset: toc; columns: 2; column-gap: 2rem; }
  .toc li { counter-increment: toc; padding: 0.25rem 0; }
  .toc li::before { content: counter(toc) "."; color: var(--text-muted); margin-right: 0.5rem; font-size: 0.85rem; }
  .toc a { color: var(--link); text-decoration: none; font-size: 0.95rem; }
  .toc a:hover { text-decoration: underline; }
  section { margin-bottom: 3rem; padding-bottom: 2rem; border-bottom: 1px solid var(--border); }
  section:last-child { border-bottom: none; }
  h2 { font-size: 1.8rem; color: var(--heading); margin-bottom: 1rem; padding-top: 1rem; }
  h3 { font-size: 1.3rem; color: var(--heading); margin: 1.5rem 0 0.75rem; }
  h4 { font-size: 1.05rem; color: var(--heading); margin: 1.25rem 0 0.5rem; }
  p { margin-bottom: 1rem; }
  ul, ol { margin-bottom: 1rem; padding-left: 1.5rem; }
  li { margin-bottom: 0.25rem; }
  a { color: var(--link); text-decoration: none; }
  a:hover { text-decoration: underline; }
  pre { background: var(--code-bg); border: 1px solid var(--border); border-radius: 6px; padding: 1rem 1.25rem; overflow-x: auto; margin: 1rem 0; font-size: 0.875rem; line-height: 1.5; }
  code { font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace; font-size: 0.875em; }
  :not(pre) > code { background: var(--inline-code-bg); padding: 0.15em 0.4em; border-radius: 3px; font-size: 0.85em; }
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
  th, td { text-align: left; padding: 0.6rem 0.8rem; border: 1px solid var(--border); }
  th { background: var(--surface); color: var(--heading); font-weight: 600; }
  tr:nth-child(even) { background: rgba(22, 27, 34, 0.5); }
  .callout { border-left: 3px solid var(--accent); background: var(--surface); padding: 1rem 1.25rem; border-radius: 0 6px 6px 0; margin: 1rem 0; }
  .callout.tip { border-left-color: var(--green); }
  .callout.warn { border-left-color: var(--orange); }
  .callout.danger { border-left-color: var(--red); }
  .callout strong { display: block; margin-bottom: 0.25rem; color: var(--heading); }
  footer { text-align: center; padding: 2rem 0; color: var(--text-muted); font-size: 0.85rem; }
  @media (max-width: 700px) { .toc ol { columns: 1; } .hero h1 { font-size: 2rem; } }
</style>
</head>
<body>

<nav style="position:sticky;top:0;z-index:100;background:var(--surface);border-bottom:1px solid var(--border);padding:0.75rem 1.5rem;display:flex;align-items:center;gap:1.5rem;font-size:0.9rem;backdrop-filter:blur(8px);">
  <a href="index.html" style="color:var(--heading);font-weight:700;text-decoration:none;">build-<span style="color:var(--link)">kg</span></a>
  <a href="documentation.html">Docs</a>
  <a href="guide.html">Guide</a>
  <a href="architecture.html" style="color:var(--heading);">Architecture</a>
  <a href="https://github.com/mergen-ai/build-kg" style="margin-left:auto;">GitHub</a>
</nav>

<div class="container">

<div class="hero">
  <h1>build-kg</h1>
  <p class="tagline">Architecture</p>
  <div class="badges">
    <span class="badge green">Apache 2.0</span>
    <span class="badge blue">Python 3.10+</span>
    <span class="badge purple">Apache AGE</span>
  </div>
</div>

<nav class="toc">
  <h2>Table of Contents</h2>
  <ol>
    <li><a href="#pipeline-overview">Pipeline Overview</a></li>
    <li><a href="#graph-ontology">Graph Ontology</a></li>
    <li><a href="#database-schema">Database Schema</a></li>
    <li><a href="#ontology-configuration">Ontology Configuration</a></li>
    <li><a href="#provision-id-extraction">Provision ID Extraction</a></li>
    <li><a href="#domain-profile-system">Domain Profile System</a></li>
    <li><a href="#batch-vs-sync-tradeoffs">Batch vs Sync Tradeoffs</a></li>
    <li><a href="#data-flow-diagram">Data Flow Diagram</a></li>
  </ol>
</nav>

<p>This document describes the design of build-kg: how data flows through the pipeline, the graph ontology system, the database schema, and the key tradeoffs in parsing strategy.</p>

<!-- Section 1: Pipeline Overview -->
<section id="pipeline-overview">
<h2>Pipeline Overview</h2>

<p>build-kg transforms any topic into a structured knowledge graph through an 8-phase pipeline. Each phase produces an artifact that feeds the next.</p>

<pre><code>Phase 0       Phase 0.5      Phase 1        Phase 2        Phase 3        Phase 4        Phase 5        Phase 6
INIT          ONTOLOGY       DISCOVER       CRAWL          CHUNK          LOAD           PARSE          VALIDATE
--------      --------       --------       --------       --------       --------       --------       --------
Set graph     Auto-gen       WebSearch      crawl.py       chunk.py       load.py        parse.py       Cypher
name, dirs    ontology       WebFetch       (Crawl4AI)     (Unstructured) (PostgreSQL)   (GPT-4o-mini)  queries
              or load
              from profile
              +----------+   +----------+   +----------+   +----------+   +-----------+  +-----------+
              | ontology |   | crawl_   |   | Markdown |   | JSON     |   | source_   |  | Apache    |
              | .yaml    |   | manifest |--&gt;| files    |--&gt;| chunks   |--&gt;| document  |-&gt;| AGE       |
              +----------+   | .json    |   | per page |   | per file |   | source_   |  | Graph DB  |
                             +----------+                               | fragment  |  +-----------+
                                                                         +-----------+</code></pre>

<h3>Phase 0: Initialize</h3>

<p>Create a working directory and choose a graph name. Graph names follow these conventions:</p>

<ul>
  <li><strong>Regulatory topics</strong>: <code>reg_&lt;country_code&gt;_&lt;domain&gt;</code> (e.g., <code>reg_sg_fb</code> for Singapore food and beverage)</li>
  <li><strong>Generic topics</strong>: <code>kg_&lt;topic&gt;</code> (e.g., <code>kg_k8s_net</code> for Kubernetes networking)</li>
</ul>

<pre><code>WORK_DIR="./pipelines/kg_k8s_net_20260218"
mkdir -p "$WORK_DIR/crawl_output" "$WORK_DIR/chunk_output"</code></pre>

<h3>Phase 0.5: Ontology Generation</h3>

<p>This phase determines the graph structure &mdash; what node types and edge types to create.</p>

<p><strong>For domain profiles with a built-in ontology</strong> (e.g., <code>food-safety</code>, <code>financial-aml</code>, <code>data-privacy</code>), the ontology is loaded directly from the profile's YAML file. No generation is needed.</p>

<p><strong>For generic topics</strong> (or profiles without an ontology section), the Claude Code skill auto-generates an ontology by analyzing the topic. The generated ontology includes:</p>

<ul>
  <li><strong>Node types</strong> with labels, descriptions, and properties (3-7 types recommended)</li>
  <li><strong>Edge types</strong> with source/target node labels and descriptions</li>
  <li><strong>Root node</strong> &mdash; the primary node type that maps 1:1 to source fragments</li>
  <li><strong>JSON schema</strong> &mdash; the expected LLM output format</li>
</ul>

<p>Example auto-generated ontology for "kubernetes networking":</p>

<pre><code>nodes:
  - label: Component
    description: "A Kubernetes networking component"
    properties: {name: string, type: string, description: string, layer: string}
  - label: Concept
    description: "A networking concept or protocol"
    properties: {name: string, description: string, category: string}
  - label: Configuration
    description: "A configuration option or setting"
    properties: {name: string, description: string, default_value: string, scope: string}
edges:
  - label: USES
    source: Component
    target: Concept
    description: "Component uses this concept"
  - label: CONFIGURES
    source: Configuration
    target: Component
    description: "Configuration applies to component"
  - label: DEPENDS_ON
    source: Component
    target: Component
    description: "Component depends on another"
root_node: Component
json_schema: |
  {
    "entities": [
      {"_label": "Component|Concept|Configuration", "name": "...", ...}
    ],
    "relationships": [
      {"_label": "USES|CONFIGURES|DEPENDS_ON", "_from_index": 0, "_to_index": 1}
    ]
  }</code></pre>

<p>The ontology is saved to <code>&lt;WORK_DIR&gt;/ontology.yaml</code> and passed to subsequent phases via the <code>--ontology</code> flag.</p>

<h3>Phase 1: Discover Sources</h3>

<p>Source discovery follows a <strong>5-round methodology</strong> designed to systematically map the knowledge landscape for a given topic:</p>

<table>
  <thead>
    <tr>
      <th>Round</th>
      <th>Name</th>
      <th>Method</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Landscape Mapping</td>
      <td>8-15 parallel web searches</td>
      <td>Identify authoritative sources and their official websites</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Deep Source Discovery</td>
      <td>Fetch main pages from each source</td>
      <td>Find individual documents, standards, specifications</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Sub-Domain Coverage Verification</td>
      <td>Cross-reference against checklist</td>
      <td>Identify gaps in topic coverage</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Gap Filling</td>
      <td>Targeted searches for uncovered areas</td>
      <td>Fill coverage gaps; aim for 90%+ coverage</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Secondary &amp; International Sources</td>
      <td>Search for supporting material</td>
      <td>Add context, completeness, and references</td>
    </tr>
  </tbody>
</table>

<p>The output of Phase 1 is a <strong>crawl manifest</strong> (<code>crawl_manifest.json</code>) that lists every source to crawl, with metadata. See <a href="manifest-format.md">manifest-format.md</a> for the full schema.</p>

<p>Sources are assigned priority tiers that determine crawl depth and page limits:</p>

<table>
  <thead>
    <tr>
      <th>Tier</th>
      <th>Description</th>
      <th>Depth</th>
      <th>Max Pages</th>
      <th>Delay</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>P1</td>
      <td>Primary authoritative sources</td>
      <td>3</td>
      <td>100</td>
      <td>1500ms</td>
    </tr>
    <tr>
      <td>P2</td>
      <td>Secondary documentation and standards</td>
      <td>2</td>
      <td>50</td>
      <td>1500ms</td>
    </tr>
    <tr>
      <td>P3</td>
      <td>Supporting guides, FAQs, tutorials</td>
      <td>1</td>
      <td>25</td>
      <td>2000ms</td>
    </tr>
    <tr>
      <td>P4</td>
      <td>Reference material, community resources</td>
      <td>1</td>
      <td>15</td>
      <td>2000ms</td>
    </tr>
  </tbody>
</table>

<h3>Phase 2: Crawl</h3>

<p>The crawler (<code>build-kg-crawl</code>) uses <a href="https://github.com/unclecode/crawl4ai">Crawl4AI</a> with a headless Chromium browser. It performs breadth-first traversal within the same domain, respecting the configured depth, page limit, and delay.</p>

<p>Each crawled page is saved as a markdown file. An optional cleaning step (<code>clean.sh</code>) removes breadcrumb navigation artifacts that appear on some government websites.</p>

<h3>Phase 3: Chunk</h3>

<p>The chunker (<code>build-kg-chunk</code>) uses the <a href="https://github.com/Unstructured-IO/unstructured">Unstructured</a> library to split documents into semantically coherent fragments. Two strategies are available:</p>

<ul>
  <li><strong><code>by_title</code></strong> (recommended): Respects document structure (headings, sections). Starts a new chunk at each heading boundary while staying under the character limit.</li>
  <li><strong><code>basic</code></strong>: Fills chunks to the maximum character limit without regard for document structure.</li>
</ul>

<p>Each chunk is saved as a JSON file with metadata including source file path, chunk index, fingerprint (SHA-256 for deduplication), and chunk position (first/middle/last/only).</p>

<h3>Phase 4: Load to Database</h3>

<p>The loader (<code>build-kg-load</code>) reads chunk JSON files and inserts them into two relational tables (<code>source_document</code> and <code>source_fragment</code>) in PostgreSQL. It matches each chunk file to its source entry in the crawl manifest by looking for the <code>source_name</code> in the file path.</p>

<p>Adjacent chunks are linked via <code>context_before</code> and <code>context_after</code> fields, giving the parser surrounding context when processing each fragment.</p>

<p>For generic topics, the <code>jurisdiction</code>, <code>authority</code>, and <code>doc_type</code> fields are nullable. Topic-specific metadata can be stored in the <code>metadata</code> JSONB column.</p>

<h3>Phase 5: Parse with LLM</h3>

<p>The parser reads fragments from <code>source_fragment</code>, sends each to GPT-4o-mini with a structured prompt, and loads the result into the Apache AGE graph. Two parser variants are available:</p>

<ul>
  <li><strong>Synchronous</strong> (<code>build-kg-parse</code>): Calls the OpenAI API in real-time. Fast turnaround, standard pricing.</li>
  <li><strong>Batch</strong> (<code>build-kg-parse-batch</code>): Uses the OpenAI Batch API. 50% cheaper, but results take 1-24 hours.</li>
</ul>

<p>The parser operates in one of two modes based on the ontology:</p>

<ul>
  <li><strong>Ontology-driven mode</strong>: When an ontology with <code>json_schema</code> is provided (via <code>--ontology</code> flag or from a domain profile), the parser uses it to generate prompts and create graph nodes/edges dynamically.</li>
  <li><strong>Legacy regulatory mode</strong>: When no ontology is present, the parser uses the hardcoded Provision/Requirement/Constraint structure.</li>
</ul>

<p>See the <a href="#batch-vs-sync-tradeoffs">Batch vs Sync Tradeoffs</a> section below.</p>

<h3>Phase 6: Validate</h3>

<p>Run Cypher queries against the completed graph to produce a report card. In ontology-driven mode, validation counts nodes per label from the ontology definition. In legacy mode, it counts Provision, Requirement, and Constraint nodes.</p>

</section>

<!-- Section 2: Graph Ontology -->
<section id="graph-ontology">
<h2>Graph Ontology</h2>

<p>build-kg supports two ontology modes: <strong>ontology-driven</strong> (generic) and <strong>regulatory</strong> (legacy).</p>

<h3>Ontology-Driven Mode (Generic)</h3>

<p>When an ontology is provided (either auto-generated or from a domain profile), the graph structure is defined entirely by the ontology configuration. Node labels, edge labels, and properties are all dynamic.</p>

<p>The LLM outputs entities and relationships in a generic JSON format:</p>

<pre><code>{
  "entities": [
    {"_label": "Component", "name": "kube-proxy", "type": "proxy", "layer": "L4"},
    {"_label": "Concept", "name": "iptables", "category": "packet filtering"}
  ],
  "relationships": [
    {"_label": "USES", "_from_index": 0, "_to_index": 1}
  ]
}</code></pre>

<p>Each entity becomes a graph vertex with the specified label and properties. Each relationship becomes a graph edge connecting the referenced entities.</p>

<h3>Regulatory Mode (Legacy / Domain Profiles)</h3>

<p>The built-in regulatory profiles (<code>food-safety</code>, <code>financial-aml</code>, <code>data-privacy</code>) define an explicit ontology with three vertex labels and two edge labels:</p>

<pre><code>Provision ──DERIVED_FROM──&gt; Requirement ──HAS_CONSTRAINT──&gt; Constraint</code></pre>

<p>More precisely, the edges point from child to parent:</p>

<ul>
  <li>A <strong>Requirement</strong> is <code>DERIVED_FROM</code> a <strong>Provision</strong> (the requirement was extracted from the provision text).</li>
  <li>A <strong>Requirement</strong> <code>HAS_CONSTRAINT</code> linking to a <strong>Constraint</strong> (the constraint makes the requirement testable).</li>
</ul>

<p>A single Provision can have multiple Requirements, and each Requirement can have multiple Constraints.</p>

<h4>Provision</h4>

<p>A provision is a fragment of regulatory text from a specific source.</p>

<table>
  <thead>
    <tr>
      <th>Property</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>id</code></td>
      <td>string</td>
      <td>Unique identifier: <code>{authority}_{provision_id}_{fragment_id_prefix}</code></td>
    </tr>
    <tr>
      <td><code>provision_id</code></td>
      <td>string</td>
      <td>Extracted regulatory ID (e.g., <code>B.01.008.2</code>, <code>21 CFR 101.61</code>) or <code>UNKNOWN</code></td>
    </tr>
    <tr>
      <td><code>text</code></td>
      <td>string</td>
      <td>The regulatory text (truncated to 500 characters)</td>
    </tr>
    <tr>
      <td><code>jurisdiction</code></td>
      <td>string</td>
      <td>ISO-style jurisdiction code (e.g., <code>CA</code>, <code>SG</code>, <code>US</code>)</td>
    </tr>
    <tr>
      <td><code>authority</code></td>
      <td>string</td>
      <td>Issuing regulatory body (e.g., <code>CFIA</code>, <code>SFA</code>, <code>FDA</code>)</td>
    </tr>
    <tr>
      <td><code>fragment_id</code></td>
      <td>string</td>
      <td>UUID linking back to <code>source_fragment</code> table</td>
    </tr>
    <tr>
      <td><code>doc_id</code></td>
      <td>string</td>
      <td>UUID linking back to <code>source_document</code> table</td>
    </tr>
    <tr>
      <td><code>created_at</code></td>
      <td>string</td>
      <td>ISO 8601 timestamp</td>
    </tr>
  </tbody>
</table>

<h4>Requirement</h4>

<p>A requirement is a single regulatory obligation extracted from a provision.</p>

<table>
  <thead>
    <tr>
      <th>Property</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>id</code></td>
      <td>string</td>
      <td>Unique identifier: <code>{provision_id}_req_{index}</code></td>
    </tr>
    <tr>
      <td><code>requirement_type</code></td>
      <td>string</td>
      <td>Domain-specific type (e.g., <code>labelling</code>, <code>consent</code>, <code>monitoring</code>)</td>
    </tr>
    <tr>
      <td><code>deontic_modality</code></td>
      <td>string</td>
      <td>One of: <code>must</code>, <code>must_not</code>, <code>may</code>, <code>should</code>, <code>should_not</code></td>
    </tr>
    <tr>
      <td><code>description</code></td>
      <td>string</td>
      <td>Human-readable description of the obligation</td>
    </tr>
    <tr>
      <td><code>applies_to_scope</code></td>
      <td>string</td>
      <td>What the requirement applies to (e.g., <code>label</code>, <code>data_controller</code>)</td>
    </tr>
  </tbody>
</table>

<h4>Constraint</h4>

<p>A constraint makes a requirement machine-testable.</p>

<table>
  <thead>
    <tr>
      <th>Property</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>id</code></td>
      <td>string</td>
      <td>Unique identifier: <code>{requirement_id}_const_{index}</code></td>
    </tr>
    <tr>
      <td><code>logic_type</code></td>
      <td>string</td>
      <td>One of: <code>threshold</code>, <code>pattern</code>, <code>enumeration</code>, <code>boolean</code></td>
    </tr>
    <tr>
      <td><code>target_signal</code></td>
      <td>string</td>
      <td>What is being measured (e.g., <code>product.sodium</code>, <code>breach.notification_time</code>)</td>
    </tr>
    <tr>
      <td><code>operator</code></td>
      <td>string</td>
      <td>Comparison operator: <code>&lt;=</code>, <code>&gt;=</code>, <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&gt;</code></td>
    </tr>
    <tr>
      <td><code>threshold</code></td>
      <td>number</td>
      <td>Numeric threshold value (for <code>threshold</code> logic type)</td>
    </tr>
    <tr>
      <td><code>unit</code></td>
      <td>string</td>
      <td>Unit of measurement (e.g., <code>mg</code>, <code>hours</code>, <code>%</code>)</td>
    </tr>
    <tr>
      <td><code>pattern</code></td>
      <td>string</td>
      <td>Regex pattern (for <code>pattern</code> logic type)</td>
    </tr>
    <tr>
      <td><code>allowed_values</code></td>
      <td>list</td>
      <td>Permitted values (for <code>enumeration</code> logic type)</td>
    </tr>
  </tbody>
</table>

<h4>Example Graph Fragment</h4>

<pre><code>(Provision {
    provision_id: "B.01.008.2",
    authority: "CFIA",
    jurisdiction: "CA"
})
    &lt;--[DERIVED_FROM]--
(Requirement {
    requirement_type: "labelling",
    deontic_modality: "must",
    description: "Sodium content must be declared on the nutrition facts table",
    applies_to_scope: "label"
})
    --[HAS_CONSTRAINT]--&gt;
(Constraint {
    logic_type: "threshold",
    target_signal: "product.sodium",
    operator: "&lt;=",
    threshold: 140,
    unit: "mg"
})</code></pre>

</section>

<!-- Section 3: Database Schema -->
<section id="database-schema">
<h2>Database Schema</h2>

<p>The relational layer in PostgreSQL bridges the raw crawled content to the Apache AGE graph. Two tables serve as the source of truth for provenance.</p>

<h3>source_document</h3>

<p>One row per crawled source document (webpage). Linked to entries in the crawl manifest.</p>

<table>
  <thead>
    <tr>
      <th>Column</th>
      <th>Type</th>
      <th>Nullable</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>doc_id</code></td>
      <td>UUID (PK)</td>
      <td>no</td>
      <td>Auto-generated primary key</td>
    </tr>
    <tr>
      <td><code>jurisdiction</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Jurisdiction code, e.g. CA, SG, US (null for generic topics)</td>
    </tr>
    <tr>
      <td><code>authority</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Source organization name (null for generic topics)</td>
    </tr>
    <tr>
      <td><code>publisher</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Publishing organization</td>
    </tr>
    <tr>
      <td><code>doc_type</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Document type, e.g. regulation, act, guidance (null for generic topics)</td>
    </tr>
    <tr>
      <td><code>title</code></td>
      <td>TEXT</td>
      <td>no</td>
      <td>Document title</td>
    </tr>
    <tr>
      <td><code>canonical_citation</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Official citation reference</td>
    </tr>
    <tr>
      <td><code>url</code></td>
      <td>TEXT</td>
      <td>no</td>
      <td>Source URL</td>
    </tr>
    <tr>
      <td><code>language</code></td>
      <td>TEXT</td>
      <td>no</td>
      <td>Language code, default <code>en</code></td>
    </tr>
    <tr>
      <td><code>filepath</code></td>
      <td>TEXT (UNIQUE)</td>
      <td>no</td>
      <td>Local file path, used for upsert deduplication</td>
    </tr>
    <tr>
      <td><code>metadata</code></td>
      <td>JSONB</td>
      <td>yes</td>
      <td>Arbitrary key-value metadata for generic topics</td>
    </tr>
    <tr>
      <td><code>retrieved_at</code></td>
      <td>TIMESTAMPTZ</td>
      <td>no</td>
      <td>When the document was crawled</td>
    </tr>
    <tr>
      <td><code>created_at</code></td>
      <td>TIMESTAMPTZ</td>
      <td>no</td>
      <td>Row creation time</td>
    </tr>
    <tr>
      <td><code>updated_at</code></td>
      <td>TIMESTAMPTZ</td>
      <td>no</td>
      <td>Last update time</td>
    </tr>
  </tbody>
</table>

<h3>source_fragment</h3>

<p>One row per chunk of a source document. This is the table the parser reads from.</p>

<table>
  <thead>
    <tr>
      <th>Column</th>
      <th>Type</th>
      <th>Nullable</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>fragment_id</code></td>
      <td>UUID (PK)</td>
      <td>no</td>
      <td>Auto-generated primary key</td>
    </tr>
    <tr>
      <td><code>doc_id</code></td>
      <td>UUID (FK)</td>
      <td>no</td>
      <td>Reference to parent source_document</td>
    </tr>
    <tr>
      <td><code>canonical_locator</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Section/chunk identifier</td>
    </tr>
    <tr>
      <td><code>excerpt</code></td>
      <td>TEXT</td>
      <td>no</td>
      <td>The actual text content of the chunk</td>
    </tr>
    <tr>
      <td><code>context_before</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Last 200 characters of the preceding chunk</td>
    </tr>
    <tr>
      <td><code>context_after</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>First 200 characters of the following chunk</td>
    </tr>
    <tr>
      <td><code>source_url</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>URL of the original source</td>
    </tr>
    <tr>
      <td><code>jurisdiction</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Inherited from source_document (null for generic)</td>
    </tr>
    <tr>
      <td><code>authority</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Inherited from source_document (null for generic)</td>
    </tr>
    <tr>
      <td><code>doc_type</code></td>
      <td>TEXT</td>
      <td>yes</td>
      <td>Inherited from source_document (null for generic)</td>
    </tr>
    <tr>
      <td><code>metadata</code></td>
      <td>JSONB</td>
      <td>yes</td>
      <td>Arbitrary key-value metadata</td>
    </tr>
    <tr>
      <td><code>created_at</code></td>
      <td>TIMESTAMPTZ</td>
      <td>no</td>
      <td>Row creation time</td>
    </tr>
  </tbody>
</table>

<h3>Indexes</h3>

<pre><code>CREATE INDEX idx_sf_doc_id ON source_fragment(doc_id);
CREATE INDEX idx_sf_jurisdiction ON source_fragment(jurisdiction);
CREATE INDEX idx_sf_authority ON source_fragment(authority);</code></pre>

</section>

<!-- Section 4: Ontology Configuration -->
<section id="ontology-configuration">
<h2>Ontology Configuration</h2>

<p>The ontology system is defined in <code>src/build_kg/domain.py</code> using Pydantic models:</p>

<h3>OntologyConfig</h3>

<pre><code>class NodeDef(BaseModel):
    label: str              # Vertex label (e.g., "Component", "Provision")
    description: str = ""   # Used in LLM prompt for guidance
    properties: Dict[str, str] = {}  # name -&gt; type (string, number, boolean, list)

class EdgeDef(BaseModel):
    label: str              # Edge label (e.g., "USES", "DERIVED_FROM")
    source: str             # Source node label
    target: str             # Target node label
    description: str = ""   # Used in LLM prompt for guidance

class OntologyConfig(BaseModel):
    description: str = ""
    nodes: List[NodeDef] = []
    edges: List[EdgeDef] = []
    root_node: str = ""     # Primary node that maps 1:1 to source fragments
    json_schema: Optional[str] = None  # Expected LLM output JSON format</code></pre>

<h3>How It's Used</h3>

<ol>
  <li><strong>Graph schema setup</strong> (<code>setup_graph.py</code>): Creates vertex labels for each node in the ontology, or falls back to <code>RegulatorySource</code>, <code>Provision</code>, <code>Requirement</code>, <code>Constraint</code>.</li>
  <li><strong>Prompt generation</strong> (<code>domain.py</code>): When <code>ontology.json_schema</code> is set, <code>build_prompt()</code> generates a prompt that describes each node type and edge type, and includes the JSON schema as the expected output format. When empty, the legacy regulatory prompt template is used.</li>
  <li><strong>Graph loading</strong> (<code>parse.py</code>, <code>parse_batch.py</code>): In ontology-driven mode, the parser creates vertices and edges dynamically from the LLM's JSON output. In legacy mode, the hardcoded Provision/Requirement/Constraint Cypher queries are used.</li>
</ol>

</section>

<!-- Section 5: Provision ID Extraction -->
<section id="provision-id-extraction">
<h2>Provision ID Extraction</h2>

<p>For regulatory domains, extracting the correct provision ID (e.g., <code>B.01.008.2</code> or <code>21 CFR 101.61</code>) is critical for linking graph nodes to their authoritative source. build-kg uses a <strong>two-stage strategy</strong>:</p>

<h3>Stage 1: Regex Extraction (Free, Fast)</h3>

<p>The <code>ProvisionIDExtractor</code> class tries multiple regex patterns in priority order:</p>

<ol>
  <li><strong>canonical_locator</strong> from chunk metadata (highest confidence, 0.95)</li>
  <li><strong>Authority-specific patterns</strong> (confidence 0.85):
    <ul>
      <li>CFIA: <code>B.01.008</code>, <code>B.01.008.2</code>, <code>D.01</code></li>
      <li>US CFR: <code>21 CFR 101.61</code>, <code>101.61</code></li>
      <li>General: <code>Section 101.61</code>, <code>Chapter 27</code>, <code>Article 15.2</code></li>
    </ul>
  </li>
  <li><strong>Generic patterns</strong> (confidence 0.70): subsection numbers, parenthetical references, schedule references</li>
</ol>

<p>Exclusion filters prevent false positives from years (2024), percentages (15%), and measurement values (100mg).</p>

<h3>Stage 2: LLM Fallback</h3>

<p>If regex fails to extract an ID, the LLM prompt includes an instruction to extract the provision ID. The parser then selects the best ID:</p>

<ol>
  <li>If regex found an ID with confidence &gt;= 0.70, use it.</li>
  <li>Else if the LLM found a non-UNKNOWN ID, use the LLM result.</li>
  <li>Else if regex found an ID with any confidence, use it (better than UNKNOWN).</li>
  <li>Otherwise, set the provision ID to <code>UNKNOWN</code>.</li>
</ol>

<p>For generic (non-regulatory) topics where provision IDs don't exist, the ID extraction is skipped entirely and auto-generated UUIDs are used instead.</p>

</section>

<!-- Section 6: Domain Profile System -->
<section id="domain-profile-system">
<h2>Domain Profile System</h2>

<p>build-kg uses YAML domain profiles to parameterize the pipeline. Profiles configure the ontology, LLM prompt, ID extraction patterns, and source discovery templates.</p>

<h3>Profile Structure</h3>

<p>A domain profile controls four aspects of the pipeline:</p>

<ol>
  <li><strong>Ontology Configuration</strong> (<code>ontology</code>): Node types, edge types, root node, and JSON schema for the knowledge graph. When present, enables ontology-driven mode. When absent (as in the <code>default</code> profile), the Claude Code skill auto-generates an ontology.</li>
  <li><strong>Parsing Configuration</strong> (<code>parsing</code>): System message, requirement types, deontic modalities, target signal examples, and scope examples used in the LLM prompt.</li>
  <li><strong>ID Extraction Patterns</strong> (<code>id_patterns</code>): Regex patterns for extracting provision IDs from regulatory text, authority-specific priority ordering, and exclusion filters.</li>
  <li><strong>Discovery Configuration</strong> (<code>discovery</code>): Search templates, sub-domain checklists, and priority tiers used by the Claude Code skill during Phase 1 source discovery.</li>
</ol>

<h3>Inheritance</h3>

<p>Profiles can inherit from a base profile using <code>extends: default</code>. Fields in the child profile override the base via deep merge. This avoids repeating universal configuration in every domain profile.</p>

<h3>Resolution Order</h3>

<p>When loading a profile by name:</p>

<ol>
  <li>If the name is a file path (ends in <code>.yaml</code>/<code>.yml</code>), load directly</li>
  <li>Otherwise, look for <code>{name}.yaml</code> in <code>src/build_kg/domains/</code></li>
</ol>

<p>The active profile is determined by:</p>

<ol>
  <li><code>--domain</code> CLI flag (highest priority)</li>
  <li><code>DOMAIN</code> environment variable</li>
  <li>Default: <code>food-safety</code></li>
</ol>

<h3>Built-in Profiles</h3>

<table>
  <thead>
    <tr>
      <th>Profile</th>
      <th>Ontology</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>default</code></td>
      <td>None (auto-generated)</td>
      <td>Generic profile for any topic</td>
    </tr>
    <tr>
      <td><code>food-safety</code></td>
      <td>Provision/Requirement/Constraint</td>
      <td>CFIA, FDA, SFA food safety and labeling</td>
    </tr>
    <tr>
      <td><code>financial-aml</code></td>
      <td>Provision/Requirement/Constraint</td>
      <td>Anti-money laundering, KYC, financial compliance</td>
    </tr>
    <tr>
      <td><code>data-privacy</code></td>
      <td>Provision/Requirement/Constraint</td>
      <td>GDPR, CCPA, data protection regulations</td>
    </tr>
  </tbody>
</table>

</section>

<!-- Section 7: Batch vs Sync Tradeoffs -->
<section id="batch-vs-sync-tradeoffs">
<h2>Batch vs Sync Tradeoffs</h2>

<table>
  <thead>
    <tr>
      <th>Factor</th>
      <th>Sync Parser (<code>build-kg-parse</code>)</th>
      <th>Batch Parser (<code>build-kg-parse-batch</code>)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Cost</strong></td>
      <td>~$0.30 per 1,000 fragments</td>
      <td>~$0.15 per 1,000 fragments (50% cheaper)</td>
    </tr>
    <tr>
      <td><strong>Latency</strong></td>
      <td>Real-time (seconds per fragment)</td>
      <td>1-24 hours for the entire batch</td>
    </tr>
    <tr>
      <td><strong>Use when</strong></td>
      <td>&lt;500 fragments, iterating quickly, debugging</td>
      <td>&gt;=500 fragments, cost-sensitive, overnight runs</td>
    </tr>
    <tr>
      <td><strong>Rate limiting</strong></td>
      <td>Controlled by <code>RATE_LIMIT_DELAY</code> (default 1s)</td>
      <td>Handled by OpenAI</td>
    </tr>
    <tr>
      <td><strong>Error handling</strong></td>
      <td>Immediate feedback per fragment</td>
      <td>Errors collected in output file</td>
    </tr>
    <tr>
      <td><strong>Resumability</strong></td>
      <td>Re-run with <code>--offset</code> to skip processed fragments</td>
      <td>Resubmit failed items</td>
    </tr>
  </tbody>
</table>

<h3>Cost Estimate Formula</h3>

<pre><code>Estimated cost = (number_of_fragments * avg_tokens_per_fragment) / 1,000,000 * price_per_million_tokens</code></pre>

<p>For GPT-4o-mini with typical text (~500 input tokens + ~300 output tokens per fragment):</p>

<ul>
  <li><strong>Sync</strong>: ~$0.00030 per fragment</li>
  <li><strong>Batch</strong>: ~$0.00015 per fragment</li>
</ul>

<h3>Recommended Workflow</h3>

<p>For a typical graph build with 1,000-5,000 fragments:</p>

<ol>
  <li>Run <code>build-kg-parse --test</code> to verify the pipeline with 5 fragments (cost: &lt; $0.01).</li>
  <li>If results look good, use <code>build-kg-parse-batch</code> for the full run.</li>
  <li>Submit the batch before end of day; process results the next morning.</li>
</ol>

</section>

<!-- Section 8: Data Flow Diagram -->
<section id="data-flow-diagram">
<h2>Data Flow Diagram</h2>

<pre><code>ontology.yaml (or profile)
        |
        v
crawl_manifest.json
        |
        v
  [build-kg-crawl]  ------&gt;  ./crawl_output/&lt;source_name&gt;/*.md
                                       |
                                       v
                              [build-kg-chunk]  ------&gt;  ./chunk_output/&lt;source_name&gt;/*_chunk_N.json
                                                                  |
                                                                  v
                                                         [build-kg-load]  ------&gt;  source_document (PostgreSQL)
                                                                                   source_fragment (PostgreSQL)
                                                                                          |
                                                                                          v
                                                                                 [build-kg-parse]  ------&gt;  Apache AGE Graph
                                                                                 (--ontology flag)          - Dynamic node labels
                                                                                                            - Dynamic edge labels
                                                                                                            - Properties from ontology</code></pre>

</section>

<footer>
  <p>build-kg &mdash; Apache 2.0 License &mdash; <a href="https://github.com/mergen-ai/build-kg">GitHub</a></p>
</footer>

</div>
</body>
</html>